{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "from geopy.distance import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"any constants you might need; some have been added for you, and some you need to fill in\"\"\"\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make sure the QUERY_DIRECTORY exists\"\"\"\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function takes the shapefile and returns an object\n",
    "    consisting of each zone, locationId and its geomtry coordinates \"\"\"\n",
    "\n",
    "def load_taxi_zones(shapefile):\n",
    "    gdf = gpd.read_file(shapefile)\n",
    "\n",
    "    taxi_zones = []\n",
    "\n",
    "    for index, row in gdf.iterrows():\n",
    "        zone = row.iloc[3]\n",
    "        locationId = row.iloc[4]\n",
    "        geometry = row.iloc[6]\n",
    "        \n",
    "        row_object = { \"zone\": zone, \"locationId\": locationId, \"geometry\": geometry }\n",
    "        taxi_zones.append(row_object)\n",
    "    \n",
    "    return taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function accepts the zone id and the taxi zones\n",
    "    and matches the zone id with its relevant coordinates \"\"\"\n",
    "\n",
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    for i in loaded_taxi_zones:\n",
    "        if i['locationId'] == zone_loc_id:\n",
    "            return i['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca606b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test - lookup_coords_for_taxi_zone_id() \"\"\"\n",
    "\n",
    "zones = [{ \"zone\": 3, \"locationId\": 1, \"geometry\": 5 }, { \"zone\": 8, \"locationId\": 7, \"geometry\": 3 }]\n",
    "assert lookup_coords_for_taxi_zone_id(1, zones)  == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function calculate the distance giving the pick up\n",
    "    point and drop off point and returns a distance integer \"\"\"\n",
    "\n",
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    pickup_latitude, pickup_longitude = from_coord\n",
    "    dropoff_latitude, dropoff_longitude = to_coord\n",
    "\n",
    "    coords = [pickup_latitude, dropoff_latitude, pickup_longitude, dropoff_longitude]\n",
    "\n",
    "    for i in coords:\n",
    "        if i < -90 or i > 90:\n",
    "            return -1\n",
    "\n",
    "    return distance((pickup_latitude, pickup_longitude), (dropoff_latitude, dropoff_longitude)).miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3c5faf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/932300510.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfrom_coord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m40.7128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m74.0060\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# New York City\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mto_coord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m34.0522\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m118.2437\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Los Angeles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_distance_with_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_coord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2449.53\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" test - calculate_distance_with_coords() \"\"\"\n",
    "\n",
    "from_coord = (37.7749, -122.4194)  # San Francisco coordinates\n",
    "to_coord = (34.0522, -118.2437)  # Los Angeles coordinates\n",
    "assert round(calculate_distance_with_coords(from_coord, to_coord), 2) == 347.37\n",
    "\n",
    "\n",
    "from_coord = (105, -122.4194)  # San Francisco coordinates\n",
    "to_coord = (34.0522, -118.2437)  # Los Angeles coordinates\n",
    "assert calculate_distance_with_coords(from_coord, to_coord) == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function adds a new column with the distance between coordinates to the Dataframe.\n",
    "    The input is a dataframe and the output is the new dataframe \"\"\"\n",
    " \n",
    "def add_distance_column(dataframe):\n",
    "    # Apply the calculate_distance_with_coords function to each row of the DataFrame\n",
    "    distances = dataframe.apply(lambda row: calculate_distance_with_coords(\n",
    "        (row[\"pickup_latitude\"], row[\"pickup_longitude\"]),\n",
    "        (row[\"dropoff_latitude\"], row[\"dropoff_longitude\"])\n",
    "    ), axis=1)\n",
    "    \n",
    "    # Add the distances as a new column to the DataFrame\n",
    "    dataframe[\"distance\"] = distances\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "912a73d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test - add_distance_column()'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" test - add_distance_column() \"\"\"\n",
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function downloads all the relevant files from the taxi webpage\n",
    "    and places it into our local directory \"\"\"\n",
    "\n",
    "def download_files(month, year):\n",
    "    formatted_month = f\"{month:02d}\"\n",
    "    current_dir = os.getcwd()\n",
    "    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{formatted_month}.parquet\"\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(f\"{current_dir}\\yellow_taxi_{year}_{formatted_month}.parquet\", \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024): \n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "years = list(range(2009, 2016))\n",
    "months = list(range(1, 13))\n",
    "\n",
    "for year in years:\n",
    "    if year < 2015:\n",
    "        for month in months:\n",
    "            download_files(month, year)\n",
    "    else:\n",
    "        for month in range(1, 7):\n",
    "            download_files(month, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function gets all the URLs from the taxi web page and returns\n",
    "    it as an array of strings \"\"\"\n",
    "\n",
    "def get_all_urls_from_taxi_page(taxi_page):\n",
    "    try:\n",
    "        response = requests.get(taxi_page)\n",
    "\n",
    "        soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "        urls = []\n",
    "\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href is not None:\n",
    "                urls.append(href)\n",
    "\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a7813aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" test for get_all_urls_from_taxi_page() \"\"\"\n",
    "\n",
    "assert len(get_all_urls_from_taxi_page(TAXI_URL)) == 483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function goes through all the URLs on the taxi web page\n",
    "    and returns only the ones ending in .parquet since we want\n",
    "    parquet files. \"\"\"\n",
    "\n",
    "def filter_taxi_parquet_urls(all_urls):\n",
    "    parquet_urls = []\n",
    "\n",
    "    if all_urls is not None:\n",
    "        for i in all_urls:\n",
    "            str = re.search('.parquet$', i)\n",
    "            if(str != None):\n",
    "                parquet_urls.append(i)\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08f83106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test for filter_taxi_parquet_urls() \"\"\"\n",
    "\n",
    "allUrlsData = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "assert len(filter_taxi_parquet_urls(allUrlsData)) == 428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function takes a URL and extracts the month from it\n",
    "    The example url can look like:\n",
    "    https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet \"\"\"\n",
    "\n",
    "def get_and_clean_month(url):\n",
    "    str = url[len(url) - 10:]\n",
    "    [month, fileType] = str.split('.')\n",
    "    return month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15238bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test for get_and_clean_month function \"\"\"\n",
    "\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet'\n",
    "assert get_and_clean_month(url) == '06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15da82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function takes a URL and extracts the year from it\n",
    "    The example url can look like:\n",
    "    https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet \"\"\"\n",
    "\n",
    "def get_and_clean_year(url):\n",
    "    str = url[len(url) - 15:]\n",
    "    [year, other] = str.split('-')\n",
    "    return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9aee9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test for get_and_clean_year function \"\"\"\n",
    "\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet'\n",
    "assert get_and_clean_year(url) == '2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd58b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This fucntion adds a new column with the distance between coordinates to the taxi Dataframe.\n",
    "    The input is a dataframe and the output is the new modified dataframe \"\"\"\n",
    " \n",
    "def add_distance_column_taxi(dataframe):\n",
    "    # Apply the calculate_distance_with_coords function to each row of the DataFrame\n",
    "    distances = dataframe.apply(lambda row: calculate_distance_with_coords(\n",
    "        (row[\"Start_Lat\"], row[\"Start_Lon\"]),\n",
    "        (row[\"End_Lat\"], row[\"End_Lon\"])\n",
    "    ), axis=1)\n",
    "    \n",
    "    # Add the distances as a new column to the DataFrame\n",
    "    dataframe[\"distance\"] = distances\n",
    "    \n",
    "    return dataframe[\"distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function collects all the parquet urls from the taxi website.\n",
    "    It will then get the actual data from the parquet files and do various forms of cleaning.\n",
    "    For example, we will remove unnecessary columns and invalid data and will return\n",
    "    one gigantic dataframe with data from every month \"\"\"\n",
    "\n",
    "def convert_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        month = get_and_clean_month(parquet_url)\n",
    "        year = get_and_clean_year(parquet_url)\n",
    "\n",
    "        cwd = os.getcwd()\n",
    "        files = os.listdir(cwd)\n",
    "\n",
    "        fileName = f\"yellow_taxi_{year}_{month}.parquet\"\n",
    "        if fileName in files :\n",
    "            dataframe = pd.read_parquet(fileName)\n",
    "            add_distance_column_taxi(dataframe)\n",
    "            all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "200776ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39m# print(taxi_data)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m taxi_data\n\u001b[1;32m---> 18\u001b[0m get_taxi_data()\n",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m, in \u001b[0;36mget_taxi_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_taxi_data\u001b[39m():\n\u001b[0;32m      5\u001b[0m     \u001b[39m# all_urls = get_all_urls_from_taxi_page(TAXI_URL)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[39m# all_parquet_urls = filter_taxi_parquet_urls(all_urls)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     all_parquet_urls \u001b[39m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-10.parquet\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     ]\n\u001b[1;32m---> 12\u001b[0m     taxi_data \u001b[39m=\u001b[39m convert_taxi_data(all_parquet_urls)\n\u001b[0;32m     14\u001b[0m     \u001b[39m# print(taxi_data)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m taxi_data\n",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m, in \u001b[0;36mconvert_taxi_data\u001b[1;34m(parquet_urls)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[39mif\u001b[39;00m fileName \u001b[39min\u001b[39;00m files :\n\u001b[0;32m     18\u001b[0m         dataframe \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(fileName)\n\u001b[1;32m---> 19\u001b[0m         add_distance_column_taxi(dataframe)\n\u001b[0;32m     20\u001b[0m         all_taxi_dataframes\u001b[39m.\u001b[39mappend(dataframe)\n\u001b[0;32m     22\u001b[0m \u001b[39m# create one gigantic dataframe with data from every month needed\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m, in \u001b[0;36madd_distance_column_taxi\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_distance_column_taxi\u001b[39m(dataframe):\n\u001b[0;32m      5\u001b[0m     \u001b[39m# Apply the calculate_distance_with_coords function to each row of the DataFrame\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     distances \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m row: calculate_distance_with_coords(\n\u001b[0;32m      7\u001b[0m         (row[\u001b[39m\"\u001b[39;49m\u001b[39mStart_Lat\u001b[39;49m\u001b[39m\"\u001b[39;49m], row[\u001b[39m\"\u001b[39;49m\u001b[39mStart_Lon\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\n\u001b[0;32m      8\u001b[0m         (row[\u001b[39m\"\u001b[39;49m\u001b[39mEnd_Lat\u001b[39;49m\u001b[39m\"\u001b[39;49m], row[\u001b[39m\"\u001b[39;49m\u001b[39mEnd_Lon\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m      9\u001b[0m     ), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# Add the distances as a new column to the DataFrame\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     dataframe[\u001b[39m\"\u001b[39m\u001b[39mdistance\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m distances\n",
      "File \u001b[1;32mc:\\Users\\sowmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   9567\u001b[0m )\n\u001b[1;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sowmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\sowmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\sowmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    911\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m, in \u001b[0;36madd_distance_column_taxi.<locals>.<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_distance_column_taxi\u001b[39m(dataframe):\n\u001b[0;32m      5\u001b[0m     \u001b[39m# Apply the calculate_distance_with_coords function to each row of the DataFrame\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     distances \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: calculate_distance_with_coords(\n\u001b[0;32m      7\u001b[0m         (row[\u001b[39m\"\u001b[39;49m\u001b[39mStart_Lat\u001b[39;49m\u001b[39m\"\u001b[39;49m], row[\u001b[39m\"\u001b[39;49m\u001b[39mStart_Lon\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\n\u001b[0;32m      8\u001b[0m         (row[\u001b[39m\"\u001b[39;49m\u001b[39mEnd_Lat\u001b[39;49m\u001b[39m\"\u001b[39;49m], row[\u001b[39m\"\u001b[39;49m\u001b[39mEnd_Lon\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m      9\u001b[0m     ), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# Add the distances as a new column to the DataFrame\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     dataframe[\u001b[39m\"\u001b[39m\u001b[39mdistance\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m distances\n",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m, in \u001b[0;36mcalculate_distance_with_coords\u001b[1;34m(from_coord, to_coord)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[39m# if (pickup_latitude or pickup_longitude or dropoff_latitude or dropoff_longitude) < -90 or (pickup_latitude or pickup_longitude or dropoff_latitude or dropoff_longitude) > 90:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m#     return -1\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[39mreturn\u001b[39;00m distance((pickup_latitude, pickup_longitude), (dropoff_latitude, dropoff_longitude))\u001b[39m.\u001b[39mmiles\n",
      "File \u001b[1;32mc:\\Users\\sowmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\geopy\\distance.py:540\u001b[0m, in \u001b[0;36mgeodesic.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_ellipsoid(kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mellipsoid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWGS-84\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m    539\u001b[0m major, minor, f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mELLIPSOID\n\u001b[1;32m--> 540\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sowmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\geopy\\distance.py:276\u001b[0m, in \u001b[0;36mDistance.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    275\u001b[0m     \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m util\u001b[39m.\u001b[39mpairwise(args):\n\u001b[1;32m--> 276\u001b[0m         kilometers \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmeasure(a, b)\n\u001b[0;32m    278\u001b[0m kilometers \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m units\u001b[39m.\u001b[39mkilometers(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kilometers \u001b[39m=\u001b[39m kilometers\n",
      "File \u001b[1;32mc:\\Users\\sowmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\geopy\\distance.py:564\u001b[0m, in \u001b[0;36mgeodesic.measure\u001b[1;34m(self, a, b)\u001b[0m\n\u001b[0;32m    559\u001b[0m lat2, lon2 \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mlatitude, b\u001b[39m.\u001b[39mlongitude\n\u001b[0;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeod, Geodesic) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    562\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeod\u001b[39m.\u001b[39ma \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mELLIPSOID[\u001b[39m0\u001b[39m] \u001b[39mand\u001b[39;00m\n\u001b[0;32m    563\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeod\u001b[39m.\u001b[39mf \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mELLIPSOID[\u001b[39m2\u001b[39m]):\n\u001b[1;32m--> 564\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeod \u001b[39m=\u001b[39m Geodesic(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mELLIPSOID[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mELLIPSOID[\u001b[39m2\u001b[39;49m])\n\u001b[0;32m    566\u001b[0m s12 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeod\u001b[39m.\u001b[39mInverse(lat1, lon1, lat2, lon2,\n\u001b[0;32m    567\u001b[0m                         Geodesic\u001b[39m.\u001b[39mDISTANCE)[\u001b[39m'\u001b[39m\u001b[39ms12\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    569\u001b[0m \u001b[39mreturn\u001b[39;00m s12\n",
      "File \u001b[1;32mc:\\Users\\sowmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\geographiclib\\geodesic.py:321\u001b[0m, in \u001b[0;36mGeodesic.__init__\u001b[1;34m(self, a, f)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A3coeff()\n\u001b[0;32m    320\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_C3coeff()\n\u001b[1;32m--> 321\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_C4coeff()\n",
      "File \u001b[1;32mc:\\Users\\sowmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\geographiclib\\geodesic.py:393\u001b[0m, in \u001b[0;36mGeodesic._C4coeff\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    369\u001b[0m coeff \u001b[39m=\u001b[39m [\n\u001b[0;32m    370\u001b[0m   \u001b[39m97\u001b[39m, \u001b[39m15015\u001b[39m,\n\u001b[0;32m    371\u001b[0m   \u001b[39m1088\u001b[39m, \u001b[39m156\u001b[39m, \u001b[39m45045\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m   \u001b[39m128\u001b[39m, \u001b[39m99099\u001b[39m,\n\u001b[0;32m    391\u001b[0m ]\n\u001b[0;32m    392\u001b[0m o \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m; k \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 393\u001b[0m \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Geodesic\u001b[39m.\u001b[39mnC4_): \u001b[39m# l is index of C4[l]\u001b[39;00m\n\u001b[0;32m    394\u001b[0m   \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Geodesic\u001b[39m.\u001b[39mnC4_ \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, l \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m): \u001b[39m# coeff of eps^j\u001b[39;00m\n\u001b[0;32m    395\u001b[0m     m \u001b[39m=\u001b[39m Geodesic\u001b[39m.\u001b[39mnC4_ \u001b[39m-\u001b[39m j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# order of polynomial in n\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" This function gets all the urls from the taxi page, specifically the parquet urls,\n",
    "    gets and cleans it, and returns the valid data \"\"\"\n",
    "\n",
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_parquet_urls = filter_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = convert_taxi_data(all_parquet_urls)\n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function first loads the uber data from the csv file. \n",
    "We then filter based on coordinates to make sure the rides are within the coordinates we want.\n",
    "We also remove trips with 0 passangers and no fares. We further remove trips with passangers above 6 as that \n",
    "is uber policy. Lastly we remove trips with no distace between dropoff and pickup. The output is the\n",
    "cleaned dataframe\"\"\"\n",
    "\n",
    "def load_and_clean_uber_data(csv_file):\n",
    "\n",
    "    # Reading in file into a data frame \n",
    "    uber_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Filter data based on pickup and dropoff latitude/longitude(40.560445, -74.242330) and (40.908524, -73.717047).\n",
    "\n",
    "    uber_data = uber_data[(uber_data[\"pickup_latitude\"] >= 40.560445) & \n",
    "                      (uber_data[\"pickup_longitude\"] >= -74.242330) & \n",
    "                      (uber_data[\"pickup_latitude\"] <= 40.908524) & \n",
    "                      (uber_data[\"pickup_longitude\"] <= -73.717047) &\n",
    "                      (uber_data[\"dropoff_latitude\"] >= 40.560445) & \n",
    "                      (uber_data[\"dropoff_longitude\"] >= -74.242330) & \n",
    "                      (uber_data[\"dropoff_latitude\"] <= 40.908524) & \n",
    "                      (uber_data[\"dropoff_longitude\"] <= -73.717047)]\n",
    "    \n",
    "    # Checking if there are any null values for pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude\n",
    "    null_drop_lat = uber_data[uber_data['dropoff_latitude'].isnull()]\n",
    "    null_drop_long = uber_data[uber_data['dropoff_longitude'].isnull()]\n",
    "    null_pick_lat= uber_data[uber_data['pickup_latitude'].isnull()]\n",
    "    null_pick_long = uber_data[uber_data['pickup_longitude'].isnull()]\n",
    "\n",
    "    # Return True, if none of the colums have null values \n",
    "\n",
    "   # if null_drop_lat.empty & null_drop_long.empty & null_pick_lat.empty & null_pick_long.empty :\n",
    "        #print(True)\n",
    "    #else:\n",
    "       # print(False)\n",
    "    \n",
    "    # Removing rows where passamger count is 0 \n",
    "    uber_data = uber_data[uber_data['passenger_count']!=0]\n",
    "\n",
    "    # Removing rows where distance is 0\n",
    "    uber_data = uber_data[uber_data['distance']==0]\n",
    "\n",
    "    # Removing rows with passanger data is abnormally large \n",
    "    uber_data = uber_data[uber_data['passenger_count']<=6]\n",
    "\n",
    "    # Checking datatypes for all columns \n",
    "    #print(uber_data.dtypes)\n",
    "\n",
    "    #Making sure pickup time is a datetime object and normalizing the name \n",
    "    uber_data ['pickup_time'] = pd.to_datetime(uber_data ['pickup_datetime'])\n",
    " \n",
    "\n",
    "\n",
    "    return uber_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "90d2ceba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24238194</td>\n",
       "      <td>2015-05-07 19:52:06.0000003</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2015-05-07 19:52:06 UTC</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-05-07 19:52:06+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27835199</td>\n",
       "      <td>2009-07-17 20:04:56.0000002</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2009-07-17 20:04:56 UTC</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-07-17 20:04:56+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44984355</td>\n",
       "      <td>2009-08-24 21:45:00.00000061</td>\n",
       "      <td>12.9</td>\n",
       "      <td>2009-08-24 21:45:00 UTC</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-08-24 21:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25894730</td>\n",
       "      <td>2009-06-26 08:22:21.0000001</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2009-06-26 08:22:21 UTC</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>3</td>\n",
       "      <td>2009-06-26 08:22:21+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17610152</td>\n",
       "      <td>2014-08-28 17:47:00.000000188</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2014-08-28 17:47:00 UTC</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-08-28 17:47:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>42598914</td>\n",
       "      <td>2012-10-28 10:49:00.00000053</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2012-10-28 10:49:00 UTC</td>\n",
       "      <td>-73.987042</td>\n",
       "      <td>40.739367</td>\n",
       "      <td>-73.986525</td>\n",
       "      <td>40.740297</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-10-28 10:49:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>16382965</td>\n",
       "      <td>2014-03-14 01:09:00.0000008</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2014-03-14 01:09:00 UTC</td>\n",
       "      <td>-73.984722</td>\n",
       "      <td>40.736837</td>\n",
       "      <td>-74.006672</td>\n",
       "      <td>40.739620</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-03-14 01:09:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>27804658</td>\n",
       "      <td>2009-06-29 00:42:00.00000078</td>\n",
       "      <td>30.9</td>\n",
       "      <td>2009-06-29 00:42:00 UTC</td>\n",
       "      <td>-73.986017</td>\n",
       "      <td>40.756487</td>\n",
       "      <td>-73.858957</td>\n",
       "      <td>40.692588</td>\n",
       "      <td>2</td>\n",
       "      <td>2009-06-29 00:42:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>20259894</td>\n",
       "      <td>2015-05-20 14:56:25.0000004</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2015-05-20 14:56:25 UTC</td>\n",
       "      <td>-73.997124</td>\n",
       "      <td>40.725452</td>\n",
       "      <td>-73.983215</td>\n",
       "      <td>40.695415</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-05-20 14:56:25+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>11951496</td>\n",
       "      <td>2010-05-15 04:08:00.00000076</td>\n",
       "      <td>14.1</td>\n",
       "      <td>2010-05-15 04:08:00 UTC</td>\n",
       "      <td>-73.984395</td>\n",
       "      <td>40.720077</td>\n",
       "      <td>-73.985508</td>\n",
       "      <td>40.768793</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-05-15 04:08:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194784 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                            key  fare_amount  \\\n",
       "0         24238194    2015-05-07 19:52:06.0000003          7.5   \n",
       "1         27835199    2009-07-17 20:04:56.0000002          7.7   \n",
       "2         44984355   2009-08-24 21:45:00.00000061         12.9   \n",
       "3         25894730    2009-06-26 08:22:21.0000001          5.3   \n",
       "4         17610152  2014-08-28 17:47:00.000000188         16.0   \n",
       "...            ...                            ...          ...   \n",
       "199995    42598914   2012-10-28 10:49:00.00000053          3.0   \n",
       "199996    16382965    2014-03-14 01:09:00.0000008          7.5   \n",
       "199997    27804658   2009-06-29 00:42:00.00000078         30.9   \n",
       "199998    20259894    2015-05-20 14:56:25.0000004         14.5   \n",
       "199999    11951496   2010-05-15 04:08:00.00000076         14.1   \n",
       "\n",
       "                pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0       2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
       "1       2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
       "2       2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n",
       "3       2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n",
       "4       2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n",
       "...                         ...               ...              ...   \n",
       "199995  2012-10-28 10:49:00 UTC        -73.987042        40.739367   \n",
       "199996  2014-03-14 01:09:00 UTC        -73.984722        40.736837   \n",
       "199997  2009-06-29 00:42:00 UTC        -73.986017        40.756487   \n",
       "199998  2015-05-20 14:56:25 UTC        -73.997124        40.725452   \n",
       "199999  2010-05-15 04:08:00 UTC        -73.984395        40.720077   \n",
       "\n",
       "        dropoff_longitude  dropoff_latitude  passenger_count  \\\n",
       "0              -73.999512         40.723217                1   \n",
       "1              -73.994710         40.750325                1   \n",
       "2              -73.962565         40.772647                1   \n",
       "3              -73.965316         40.803349                3   \n",
       "4              -73.973082         40.761247                5   \n",
       "...                   ...               ...              ...   \n",
       "199995         -73.986525         40.740297                1   \n",
       "199996         -74.006672         40.739620                1   \n",
       "199997         -73.858957         40.692588                2   \n",
       "199998         -73.983215         40.695415                1   \n",
       "199999         -73.985508         40.768793                1   \n",
       "\n",
       "                     pickup_time  \n",
       "0      2015-05-07 19:52:06+00:00  \n",
       "1      2009-07-17 20:04:56+00:00  \n",
       "2      2009-08-24 21:45:00+00:00  \n",
       "3      2009-06-26 08:22:21+00:00  \n",
       "4      2014-08-28 17:47:00+00:00  \n",
       "...                          ...  \n",
       "199995 2012-10-28 10:49:00+00:00  \n",
       "199996 2014-03-14 01:09:00+00:00  \n",
       "199997 2009-06-29 00:42:00+00:00  \n",
       "199998 2015-05-20 14:56:25+00:00  \n",
       "199999 2010-05-15 04:08:00+00:00  \n",
       "\n",
       "[194784 rows x 10 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_and_clean_uber_data(\"uber_rides_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We use the add distance column fcuntion we had defined before to add a new column with the distance \n",
    "of the ride to our uber data. We also drop columns where the distance of the ride is ==0\"\"\"\n",
    "\n",
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(\"uber_rides_sample.csv\")\n",
    "    add_distance_column(uber_dataframe)\n",
    "    uber_dataframe = uber_dataframe.drop(index=uber_dataframe[uber_dataframe['distance'] == 0].index)\n",
    "    return uber_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "826a4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unnecessary columns \n",
    "final_uber_data = final_uber_data.drop('Unnamed: 0', axis=1)\n",
    "final_uber_data = final_uber_data.drop('key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "339997e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_time</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24238194</td>\n",
       "      <td>2015-05-07 19:52:06.0000003</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2015-05-07 19:52:06 UTC</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-05-07 19:52:06+00:00</td>\n",
       "      <td>1.044594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27835199</td>\n",
       "      <td>2009-07-17 20:04:56.0000002</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2009-07-17 20:04:56 UTC</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-07-17 20:04:56+00:00</td>\n",
       "      <td>1.525071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44984355</td>\n",
       "      <td>2009-08-24 21:45:00.00000061</td>\n",
       "      <td>12.9</td>\n",
       "      <td>2009-08-24 21:45:00 UTC</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-08-24 21:45:00+00:00</td>\n",
       "      <td>3.131464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25894730</td>\n",
       "      <td>2009-06-26 08:22:21.0000001</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2009-06-26 08:22:21 UTC</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>3</td>\n",
       "      <td>2009-06-26 08:22:21+00:00</td>\n",
       "      <td>1.032372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17610152</td>\n",
       "      <td>2014-08-28 17:47:00.000000188</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2014-08-28 17:47:00 UTC</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-08-28 17:47:00+00:00</td>\n",
       "      <td>2.786061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>42598914</td>\n",
       "      <td>2012-10-28 10:49:00.00000053</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2012-10-28 10:49:00 UTC</td>\n",
       "      <td>-73.987042</td>\n",
       "      <td>40.739367</td>\n",
       "      <td>-73.986525</td>\n",
       "      <td>40.740297</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-10-28 10:49:00+00:00</td>\n",
       "      <td>0.069673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>16382965</td>\n",
       "      <td>2014-03-14 01:09:00.0000008</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2014-03-14 01:09:00 UTC</td>\n",
       "      <td>-73.984722</td>\n",
       "      <td>40.736837</td>\n",
       "      <td>-74.006672</td>\n",
       "      <td>40.739620</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-03-14 01:09:00+00:00</td>\n",
       "      <td>1.167951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>27804658</td>\n",
       "      <td>2009-06-29 00:42:00.00000078</td>\n",
       "      <td>30.9</td>\n",
       "      <td>2009-06-29 00:42:00 UTC</td>\n",
       "      <td>-73.986017</td>\n",
       "      <td>40.756487</td>\n",
       "      <td>-73.858957</td>\n",
       "      <td>40.692588</td>\n",
       "      <td>2</td>\n",
       "      <td>2009-06-29 00:42:00+00:00</td>\n",
       "      <td>7.995752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>20259894</td>\n",
       "      <td>2015-05-20 14:56:25.0000004</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2015-05-20 14:56:25 UTC</td>\n",
       "      <td>-73.997124</td>\n",
       "      <td>40.725452</td>\n",
       "      <td>-73.983215</td>\n",
       "      <td>40.695415</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-05-20 14:56:25+00:00</td>\n",
       "      <td>2.197512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>11951496</td>\n",
       "      <td>2010-05-15 04:08:00.00000076</td>\n",
       "      <td>14.1</td>\n",
       "      <td>2010-05-15 04:08:00 UTC</td>\n",
       "      <td>-73.984395</td>\n",
       "      <td>40.720077</td>\n",
       "      <td>-73.985508</td>\n",
       "      <td>40.768793</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-05-15 04:08:00+00:00</td>\n",
       "      <td>3.362040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192827 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                            key  fare_amount  \\\n",
       "0         24238194    2015-05-07 19:52:06.0000003          7.5   \n",
       "1         27835199    2009-07-17 20:04:56.0000002          7.7   \n",
       "2         44984355   2009-08-24 21:45:00.00000061         12.9   \n",
       "3         25894730    2009-06-26 08:22:21.0000001          5.3   \n",
       "4         17610152  2014-08-28 17:47:00.000000188         16.0   \n",
       "...            ...                            ...          ...   \n",
       "199995    42598914   2012-10-28 10:49:00.00000053          3.0   \n",
       "199996    16382965    2014-03-14 01:09:00.0000008          7.5   \n",
       "199997    27804658   2009-06-29 00:42:00.00000078         30.9   \n",
       "199998    20259894    2015-05-20 14:56:25.0000004         14.5   \n",
       "199999    11951496   2010-05-15 04:08:00.00000076         14.1   \n",
       "\n",
       "                pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0       2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
       "1       2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
       "2       2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n",
       "3       2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n",
       "4       2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n",
       "...                         ...               ...              ...   \n",
       "199995  2012-10-28 10:49:00 UTC        -73.987042        40.739367   \n",
       "199996  2014-03-14 01:09:00 UTC        -73.984722        40.736837   \n",
       "199997  2009-06-29 00:42:00 UTC        -73.986017        40.756487   \n",
       "199998  2015-05-20 14:56:25 UTC        -73.997124        40.725452   \n",
       "199999  2010-05-15 04:08:00 UTC        -73.984395        40.720077   \n",
       "\n",
       "        dropoff_longitude  dropoff_latitude  passenger_count  \\\n",
       "0              -73.999512         40.723217                1   \n",
       "1              -73.994710         40.750325                1   \n",
       "2              -73.962565         40.772647                1   \n",
       "3              -73.965316         40.803349                3   \n",
       "4              -73.973082         40.761247                5   \n",
       "...                   ...               ...              ...   \n",
       "199995         -73.986525         40.740297                1   \n",
       "199996         -74.006672         40.739620                1   \n",
       "199997         -73.858957         40.692588                2   \n",
       "199998         -73.983215         40.695415                1   \n",
       "199999         -73.985508         40.768793                1   \n",
       "\n",
       "                     pickup_time  distance  \n",
       "0      2015-05-07 19:52:06+00:00  1.044594  \n",
       "1      2009-07-17 20:04:56+00:00  1.525071  \n",
       "2      2009-08-24 21:45:00+00:00  3.131464  \n",
       "3      2009-06-26 08:22:21+00:00  1.032372  \n",
       "4      2014-08-28 17:47:00+00:00  2.786061  \n",
       "...                          ...       ...  \n",
       "199995 2012-10-28 10:49:00+00:00  0.069673  \n",
       "199996 2014-03-14 01:09:00+00:00  1.167951  \n",
       "199997 2009-06-29 00:42:00+00:00  7.995752  \n",
       "199998 2015-05-20 14:56:25+00:00  2.197512  \n",
       "199999 2010-05-15 04:08:00+00:00  3.362040  \n",
       "\n",
       "[192827 rows x 11 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_uber_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function takes all the weather files, iterates through them and merges them \n",
    "into one dataframe. The output is the combined dataframe\"\"\"\n",
    "\n",
    "def get_all_weather_csvs():\n",
    "    years = list(range(2009, 2016))\n",
    "\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Iterate over the weather files\n",
    "    for year in years:\n",
    "        filepath = f\"{year}_weather.csv\"\n",
    "        df = pd.read_csv(filepath)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all the dataframes into a single dataframe\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "fb7e80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function first loads the uber data from the csv file. \n",
    "We then filter based on coordinates to make sure the rides are within the coordinates we want.\n",
    "We also remove trips with 0 passangers and no fares. We further remove trips with passangers above 6 as that \n",
    "is uber policy. Lastly we remove trips with no distace between dropoff and pickup. The output is the\n",
    "cleaned dataframe\"\"\"\n",
    "\n",
    "def load_and_clean_weather_data():\n",
    "\n",
    "    df = get_all_weather_csvs()\n",
    "\n",
    "    df1 = df[['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'NAME','HourlyPrecipitation','HourlyWindGustSpeed', 'HourlyWindSpeed', 'DailyAverageWindSpeed','DailyPrecipitation']]\n",
    "    df2 = df1.dropna(subset=['HourlyPrecipitation', 'HourlyWindGustSpeed'])\n",
    "\n",
    "    #column_types = df2.dtypes\n",
    "\n",
    "    #print(column_types)\n",
    "\n",
    "    # we see that the averages for wind speed and precipitation are null for all values so we can drop the columns \n",
    "\n",
    "    # We also doing need the hourly wind gust speed as we will be using the hourly wind speed, we can drop that column as well\n",
    "\n",
    "    df2 = df2.drop(columns=['DailyAverageWindSpeed','DailyPrecipitation', 'HourlyWindGustSpeed','LATITUDE', 'LONGITUDE'])\n",
    "    df2['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "    df2\n",
    "\n",
    "    # Removing all rows where Hourly preicipitation has the value \"T\" as we do not need to measure trace amounts \n",
    "\n",
    "    df3 = df2[df2['HourlyPrecipitation'] != \"T\"]\n",
    "\n",
    "    df4 = df3.drop(columns=[\"STATION\"])\n",
    "\n",
    "    df4 = df4.reset_index()\n",
    "\n",
    "    df4['DATE'] = df4['DATE'].apply(lambda x: x.to_pydatetime())\n",
    "\n",
    "    df4['DATE'] = pd.to_datetime(df4['DATE'])\n",
    "\n",
    "    df4['HourlyPrecipitation'] = df4['HourlyPrecipitation'].str.replace(r'(\\d+)\\s*[sS]$', r'\\1', regex=True)\n",
    "    \n",
    "    # convert column \"A\" from object to float\n",
    "    df4['HourlyPrecipitation'] = df4['HourlyPrecipitation'].astype(float)\n",
    "\n",
    "    Weather_Data = df4.drop('index', axis=1)\n",
    "\n",
    "    return  Weather_Data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7a3a9156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-06 20:00:00</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-06 23:38:00</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>0.02</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-07 02:51:00</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>0.09</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-07 03:51:00</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>0.06</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-07 04:51:00</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>0.07</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7098</th>\n",
       "      <td>2015-12-29 10:51:00</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>0.02</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7099</th>\n",
       "      <td>2015-12-29 11:33:00</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>0.02</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>2015-12-29 11:51:00</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>0.02</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7101</th>\n",
       "      <td>2015-12-31 11:51:00</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7102</th>\n",
       "      <td>2015-12-31 20:51:00</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7103 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    DATE                         NAME  HourlyPrecipitation  \\\n",
       "0    2009-01-06 20:00:00  NY CITY CENTRAL PARK, NY US                 0.01   \n",
       "1    2009-01-06 23:38:00  NY CITY CENTRAL PARK, NY US                 0.02   \n",
       "2    2009-01-07 02:51:00  NY CITY CENTRAL PARK, NY US                 0.09   \n",
       "3    2009-01-07 03:51:00  NY CITY CENTRAL PARK, NY US                 0.06   \n",
       "4    2009-01-07 04:51:00  NY CITY CENTRAL PARK, NY US                 0.07   \n",
       "...                  ...                          ...                  ...   \n",
       "7098 2015-12-29 10:51:00  NY CITY CENTRAL PARK, NY US                 0.02   \n",
       "7099 2015-12-29 11:33:00  NY CITY CENTRAL PARK, NY US                 0.02   \n",
       "7100 2015-12-29 11:51:00  NY CITY CENTRAL PARK, NY US                 0.02   \n",
       "7101 2015-12-31 11:51:00  NY CITY CENTRAL PARK, NY US                 0.00   \n",
       "7102 2015-12-31 20:51:00  NY CITY CENTRAL PARK, NY US                 0.00   \n",
       "\n",
       "      HourlyWindSpeed  \n",
       "0                10.0  \n",
       "1                11.0  \n",
       "2                13.0  \n",
       "3                15.0  \n",
       "4                16.0  \n",
       "...               ...  \n",
       "7098             10.0  \n",
       "7099              8.0  \n",
       "7100              6.0  \n",
       "7101              9.0  \n",
       "7102             10.0  \n",
       "\n",
       "[7103 rows x 4 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Roll up the data to daily\"\"\"\n",
    "def clean_month_weather_data_daily():\n",
    "\n",
    "    daily_data = load_and_clean_weather_data()\n",
    "\n",
    "    daily_data_final = daily_data.groupby([daily_data['DATE'].dt.year, daily_data['DATE'].dt.month, daily_data['DATE'].dt.day]).sum()[['HourlyPrecipitation', \"HourlyWindSpeed\" ]]\n",
    "\n",
    "    daily_data_final = daily_data_final.rename_axis(index=['Year', 'Month', 'Day'])\n",
    "\n",
    "\n",
    "    \n",
    "    return daily_data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e3e6f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly():\n",
    "\n",
    "    hourly_data = load_and_clean_weather_data()\n",
    "\n",
    "    hourly_data_final = hourly_data.groupby([hourly_data['DATE'].dt.year, hourly_data['DATE'].dt.month, hourly_data['DATE'].dt.day, hourly_data['DATE'].dt.hour]).sum()[['HourlyPrecipitation', \"HourlyWindSpeed\" ]]\n",
    "\n",
    "    hourly_data_final = hourly_data_final.rename_axis(index=['Year', 'Month', 'Day', 'Hour'])\n",
    "    \n",
    "    return hourly_data_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_and_clean_weather_data():\\n    weather_csv_files = get_all_weather_csvs()\\n    \\n    hourly_dataframes = []\\n    daily_dataframes = []\\n        \\n    for csv_file in weather_csv_files:\\n        hourly_dataframe = clean_month_weather_data_hourly()\\n        daily_dataframe = clean_month_weather_data_daily()\\n        hourly_dataframes.append(hourly_dataframe)\\n        daily_dataframes.append(daily_dataframe)\\n        \\n    # create two dataframes with hourly & daily data from every month\\n    hourly_data = pd.concat(hourly_dataframes)\\n    daily_data = pd.concat(daily_dataframes)\\n    \\n    return hourly_data, daily_data\\n    '"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" I dont think this is needed as my daily and hourly functions retunr the datframes that we need\"\"\"\n",
    "\"\"\"\n",
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs()\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly()\n",
    "        daily_dataframe = clean_month_weather_data_daily()\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "48216557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2009</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>20</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.02</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">7</th>\n",
       "      <th>2</th>\n",
       "      <td>0.09</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.06</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.07</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     HourlyPrecipitation  HourlyWindSpeed\n",
       "Year Month Day Hour                                      \n",
       "2009 1     6   20                   0.01             10.0\n",
       "               23                   0.02             11.0\n",
       "           7   2                    0.09             13.0\n",
       "               3                    0.06             15.0\n",
       "               4                    0.07             16.0"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data = clean_month_weather_data_hourly()\n",
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "/var/folders/4y/mgwhf9fd22746wblcfjds5gc0000gn/T/ipykernel_21281/3776051388.py:13: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2009</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>6</th>\n",
       "      <td>0.03</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.13</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.06</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.26</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.69</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                HourlyPrecipitation  HourlyWindSpeed\n",
       "Year Month Day                                      \n",
       "2009 1     6                   0.03             21.0\n",
       "           7                   1.13            224.0\n",
       "           10                  0.06             48.0\n",
       "           11                  0.26             67.0\n",
       "           17                  0.69              7.0"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data = clean_month_weather_data_daily()\n",
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS HOURLY_WEATHER (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        year INTEGER,\n",
    "        month INTEGER,\n",
    "        day INTEGER,\n",
    "        precipitation REAL,\n",
    "        wind REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS DAILY_WEATHER (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        year INTEGER,\n",
    "        month INTEGER,\n",
    "        day INTEGER,\n",
    "        precipitation REAL,\n",
    "        wind REAL\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS TAXI_TRIPS (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        pickup_datetime TEXT,\n",
    "        pickup_longitude REAL,\n",
    "        pickup_latitude REAL,\n",
    "        dropoff_longitude REAL,\n",
    "        dropoff_latitude REAL,\n",
    "        fare_amount REAL,\n",
    "        distance REAL,\n",
    "        passenger_count INTEGER,\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS UBER_TRIPS (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        pickup_datetime TEXT,\n",
    "        pickup_longitude REAL,\n",
    "        pickup_latitude REAL,\n",
    "        dropoff_longitude REAL,\n",
    "        dropoff_latitude REAL,\n",
    "        fare_amount REAL,\n",
    "        distance REAL,\n",
    "        passenger_count INTEGER,\n",
    "    );\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d659ea492a6423f437f8c825d2ef59ba06a7f23250fb4bfa35b0a006bd446c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
