{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from geopy.distance import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"any constants you might need; some have been added for you, and some you need to fill in\"\"\"\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make sure the QUERY_DIRECTORY exists\"\"\"\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function takes the shapefile and returns an object\n",
    "    consisting of each zone, locationId and its geomtry coordinates \"\"\"\n",
    "\n",
    "def load_taxi_zones(shapefile: str) -> dict:\n",
    "    gdf = gpd.read_file(shapefile)\n",
    "\n",
    "    taxi_zones = []\n",
    "\n",
    "    for index, row in gdf.iterrows():\n",
    "        zone = row.iloc[3]\n",
    "        locationId = row.iloc[4]\n",
    "        geometry = row.iloc[6]\n",
    "        \n",
    "        row_object = { \"zone\": zone, \"locationId\": locationId, \"geometry\": geometry }\n",
    "        taxi_zones.append(row_object)\n",
    "    \n",
    "    return taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function accepts the zone id and the taxi zones\n",
    "    and matches the zone id with its relevant coordinates \"\"\"\n",
    "\n",
    "def lookup_coords_for_taxi_zone_id(zone_loc_id: int, loaded_taxi_zones: list) -> int:\n",
    "    for i in loaded_taxi_zones:\n",
    "        if i['locationId'] == zone_loc_id:\n",
    "            return i['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca606b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test - lookup_coords_for_taxi_zone_id() \"\"\"\n",
    "\n",
    "zones = [{ \"zone\": 3, \"locationId\": 1, \"geometry\": 5 }, { \"zone\": 8, \"locationId\": 7, \"geometry\": 3 }]\n",
    "assert lookup_coords_for_taxi_zone_id(1, zones)  == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function calculate the distance giving the pick up\n",
    "    point and drop off point and returns a distance integer \"\"\"\n",
    "\n",
    "def calculate_distance_with_coords(from_coord: tuple, to_coord: tuple) -> int:\n",
    "    pickup_latitude, pickup_longitude = from_coord\n",
    "    dropoff_latitude, dropoff_longitude = to_coord\n",
    "\n",
    "    coords = [pickup_latitude, dropoff_latitude, pickup_longitude, dropoff_longitude]\n",
    "\n",
    "    for i in coords:\n",
    "        if i < -90 or i > 90:\n",
    "            return -1\n",
    "\n",
    "    return distance((pickup_latitude, pickup_longitude), (dropoff_latitude, dropoff_longitude)).miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test - calculate_distance_with_coords() \"\"\"\n",
    "\n",
    "from_coord = (37.7749, -122.4194)  # San Francisco coordinates\n",
    "to_coord = (34.0522, -118.2437)  # Los Angeles coordinates\n",
    "assert round(calculate_distance_with_coords(from_coord, to_coord), 2) == 347.37\n",
    "\n",
    "\n",
    "from_coord = (105, -122.4194)  # San Francisco coordinates\n",
    "to_coord = (34.0522, -118.2437)  # Los Angeles coordinates\n",
    "assert calculate_distance_with_coords(from_coord, to_coord) == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function adds a new column with the distance between coordinates to the Dataframe.\n",
    "    The input is a dataframe and the output is the new dataframe \"\"\"\n",
    " \n",
    "def add_distance_column(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Apply the calculate_distance_with_coords function to each row of the DataFrame\n",
    "    distances = dataframe.apply(lambda row: calculate_distance_with_coords(\n",
    "        (row[\"pickup_latitude\"], row[\"pickup_longitude\"]),\n",
    "        (row[\"dropoff_latitude\"], row[\"dropoff_longitude\"])\n",
    "    ), axis=1)\n",
    "    \n",
    "    # Add the distances as a new column to the DataFrame\n",
    "    dataframe[\"distance\"] = distances\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test - add_distance_column() \"\"\"\n",
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function downloads all the relevant files from the taxi webpage\n",
    "    and places it into our local directory \"\"\"\n",
    "\n",
    "def download_files(month: int, year: int):\n",
    "    formatted_month = f\"{month:02d}\"\n",
    "    current_dir = os.getcwd()\n",
    "    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{formatted_month}.parquet\"\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(f\"{current_dir}\\yellow_taxi_{year}_{formatted_month}.parquet\", \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024): \n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "years = list(range(2009, 2016))\n",
    "months = list(range(1, 13))\n",
    "\n",
    "for year in years:\n",
    "    if year < 2015:\n",
    "        for month in months:\n",
    "            download_files(month, year)\n",
    "    else:\n",
    "        for month in range(1, 7):\n",
    "            download_files(month, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function gets all the URLs from the taxi web page and returns\n",
    "    it as an array of strings \"\"\"\n",
    "\n",
    "def get_all_urls_from_taxi_page(taxi_page: str) -> list[str]:\n",
    "    try:\n",
    "        response = requests.get(taxi_page)\n",
    "\n",
    "        soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "        urls = []\n",
    "\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href is not None:\n",
    "                urls.append(href)\n",
    "\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7813aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" test for get_all_urls_from_taxi_page() \"\"\"\n",
    "\n",
    "assert len(get_all_urls_from_taxi_page(TAXI_URL)) == 483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function goes through all the URLs on the taxi web page\n",
    "    and returns only the ones ending in .parquet since we want\n",
    "    parquet files and also the ones from the years 2009 to 2015\n",
    "    to avoid iterating through unecessary files. \"\"\"\n",
    "\n",
    "def filter_taxi_parquet_urls(all_urls: list[str]) -> list[str]:\n",
    "    parquet_urls = []\n",
    "    years = list(range(2009, 2016))\n",
    "\n",
    "    if all_urls is not None:\n",
    "        for i in all_urls:\n",
    "            str = re.search('.parquet$', i)\n",
    "\n",
    "            if(str != None and \"yellow_tripdata\" in i):\n",
    "                year = int(i.split(\"_\")[2][:4])\n",
    "\n",
    "                if year in years:\n",
    "                    parquet_urls.append(i)\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f83106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test for filter_taxi_parquet_urls() \"\"\"\n",
    "\n",
    "allUrlsData = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "assert len(filter_taxi_parquet_urls(allUrlsData)) == 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function takes a URL and extracts the month from it\n",
    "    The example url can look like:\n",
    "    https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet \"\"\"\n",
    "\n",
    "def get_and_clean_month(url: str) -> str:\n",
    "    str = url[len(url) - 10:]\n",
    "    [month, fileType] = str.split('.')\n",
    "    return month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15238bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test for get_and_clean_month function \"\"\"\n",
    "\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet'\n",
    "assert get_and_clean_month(url) == '06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15da82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function takes a URL and extracts the year from it\n",
    "    The example url can look like:\n",
    "    https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet \"\"\"\n",
    "\n",
    "def get_and_clean_year(url: str) -> str:\n",
    "    str = url[len(url) - 15:]\n",
    "    [year, other] = str.split('-')\n",
    "    return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test for get_and_clean_year function \"\"\"\n",
    "\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet'\n",
    "assert get_and_clean_year(url) == '2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This fucntion adds a new column with the distance between coordinates to the taxi Dataframe.\n",
    "    The input is a dataframe and the output is the new modified dataframe \"\"\"\n",
    " \n",
    "def add_distance_column_taxi(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Apply the calculate_distance_with_coords function to each row of the DataFrame\n",
    "    distances = dataframe.apply(lambda row: calculate_distance_with_coords(\n",
    "        (row[\"Start_Lat\"], row[\"Start_Lon\"]),\n",
    "        (row[\"End_Lat\"], row[\"End_Lon\"])\n",
    "    ), axis=1)\n",
    "    \n",
    "    # Add the distances as a new column to the DataFrame\n",
    "    dataframe[\"distance\"] = distances\n",
    "    \n",
    "    return dataframe[\"distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function collects all the parquet urls from the taxi website.\n",
    "    It will then get the actual data from the parquet files and do various forms of cleaning.\n",
    "    For example, we will remove unnecessary columns and invalid data and will return\n",
    "    one gigantic dataframe with data from every month \"\"\"\n",
    "\n",
    "def convert_taxi_data(parquet_urls: list[str]) -> pd.DataFrame:\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        month = get_and_clean_month(parquet_url)\n",
    "        year = get_and_clean_year(parquet_url)\n",
    "\n",
    "        cwd = os.getcwd()\n",
    "        files = os.listdir(cwd)\n",
    "\n",
    "        fileName = f\"yellow_taxi_{year}_{month}.parquet\"\n",
    "        if fileName in files :\n",
    "\n",
    "            dataframe = pd.read_parquet(fileName)\n",
    "            sample_dataframe = dataframe.sample(n=20000)\n",
    "            all_taxi_dataframes.append(sample_dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function gets all the urls from the taxi page, specifically the parquet urls,\n",
    "    gets and cleans it, and returns the valid data \"\"\"\n",
    "\n",
    "def get_taxi_data() -> pd.DataFrame:\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_parquet_urls = filter_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = convert_taxi_data(all_parquet_urls)\n",
    "\n",
    "    return taxi_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac161e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance','PULocationID', 'DOLocationID','fare_amount','tip_amount','total_amount','pickup_datetime', 'dropoff_datetime', 'pickup_longitude', 'pickup_latitude',  'dropoff_longitude', 'dropoff_latitude', 'Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime', 'Passenger_Count', 'Trip_Distance', 'Start_Lon', 'Start_Lat',  'End_Lon', 'End_Lat',  'Fare_Amt', 'Tip_Amt', 'Total_Amt']\n",
    "df_selected = taxi_data[selected_cols]\n",
    "# list of column pairs to join\n",
    "column_pairs = [(\"tpep_pickup_datetime\", 'pickup_datetime'), \n",
    "                (\"tpep_dropoff_datetime\", 'dropoff_datetime'),\n",
    "                ('Trip_Distance', 'trip_distance'),\n",
    "                ('Passenger_Count', 'passenger_count'),\n",
    "                ('Start_Lon', 'PULocationID'),\n",
    "                ('Start_Lat', 'PULocationID'),\n",
    "                ('End_Lon', 'DOLocationID'),\n",
    "                ('End_Lat', 'DOLocationID'),\n",
    "                ('Fare_Amt', 'fare_amount'),\n",
    "                ('Tip_Amt', 'tip_amount'),\n",
    "                ('Total_Amt', 'total_amount')]\n",
    "\n",
    "# loop over column pairs and join them\n",
    "for pair in column_pairs:\n",
    "    # fill missing values in the first column with values from the second column\n",
    "    df_selected[pair[0]] = df_selected[pair[0]].fillna(df_selected[pair[1]])\n",
    "    # drop the second column\n",
    "    df_selected_final = df_selected.drop(pair[1], axis=1)\n",
    "\n",
    "df_selected_final = df_selected_final.drop(['pickup_datetime',\t'dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount', 'pickup_longitude',\t'pickup_latitude',\t'dropoff_longitude'\t,'dropoff_latitude'], axis=1)\n",
    "\n",
    "\n",
    "column_pairs = [(\"tpep_pickup_datetime\", 'Trip_Pickup_DateTime'), \n",
    "                (\"tpep_dropoff_datetime\", 'Trip_Dropoff_DateTime')]\n",
    "\n",
    "\n",
    "# loop over column pairs and join them\n",
    "for pair in column_pairs:\n",
    "    # fill missing values in the first column with values from the second column\n",
    "    df_selected_final[pair[0]] = df_selected_final[pair[0]].fillna(df_selected_final[pair[1]])\n",
    "    # drop the second column\n",
    "    df_selected_final = df_selected_final.drop(pair[1], axis=1)\n",
    "\n",
    "df_selected_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We then filter based on coordinates to make sure the rides are within the coordinates we want.\n",
    "We also remove trips with 0 passangers and no fares. We further remove trips with passangers above 6 as that \n",
    "is uber policy. Lastly we remove trips with no distace between dropoff and pickup. The output is the\n",
    "cleaned dataframe\n",
    "'''\n",
    "\n",
    "df_selected_final = df_selected_final[(df_selected_final[\"Start_Lat\"] >= 40.560445) & \n",
    "                                      (df_selected_final[\"Start_Lon\"] >= -74.242330) & \n",
    "                                      (df_selected_final[\"Start_Lat\"] <= 40.908524) & \n",
    "                                      (df_selected_final[\"Start_Lon\"] <= -73.717047) &\n",
    "                                      (df_selected_final[\"End_Lat\"] >= 40.560445) & \n",
    "                                      (df_selected_final[\"End_Lon\"] >= -74.242330) & \n",
    "                                      (df_selected_final[\"End_Lat\"] <= 40.908524) & \n",
    "                                      (df_selected_final[\"End_Lon\"] <= -73.717047)]\n",
    "\n",
    "    # Removing rows where distance is 0\n",
    "    uber_data = uber_data[uber_data['distance']==0]\n",
    "\n",
    "df_selected_final = df_selected_final[df_selected_final['Passenger_Count'] != 0]\n",
    "\n",
    "add_distance_column_taxi(df_selected_final)\n",
    "\n",
    "df_selected_final = df_selected_final.drop(index=df_selected_final[df_selected_final['distance'] == 0].index)\n",
    "\n",
    "df_selected_final = df_selected_final[df_selected_final['Passenger_Count']<=6.0]\n",
    "df_selected_final = df_selected_final.reset_index(drop=True)\n",
    "df_selected_final = df_selected_final.rename(columns={\n",
    "    \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "    \"Passenger_Count\": \"Passenger_Count\",\n",
    "    \"Trip_Distance\": \"Trip_Distance\",\n",
    "    \"Start_Lon\": \"Start_Lon\",\n",
    "    \"Start_Lat\": \"Start_Lat\",\n",
    "    \"End_Lon\": \"End_Lon\",\n",
    "    \"End_Lat\": \"End_Lat\",\n",
    "    \"Fare_Amt\": \"Fare_Amt\",\n",
    "    \"Tip_Amt\": \"Tip_Amt\",\n",
    "    \"Total_Amt\": \"Total_Amt\",\n",
    "    \"distance\": \"distance\"\n",
    "})\n",
    "\n",
    "df_selected_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Taxi_Data = df_selected_final.copy()\n",
    "Taxi_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function first loads the uber data from the csv file. \n",
    "We then filter based on coordinates to make sure the rides are within the coordinates we want.\n",
    "We also remove trips with 0 passangers and no fares. We further remove trips with passangers above 6 as that \n",
    "is uber policy. Lastly we remove trips with no distace between dropoff and pickup. The output is the\n",
    "cleaned dataframe\"\"\"\n",
    "\n",
    "def load_and_clean_uber_data(csv_file):\n",
    "\n",
    "    # Reading in file into a data frame \n",
    "    uber_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Filter data based on pickup and dropoff latitude/longitude(40.560445, -74.242330) and (40.908524, -73.717047).\n",
    "\n",
    "    uber_data = uber_data[(uber_data[\"pickup_latitude\"] >= 40.560445) & \n",
    "                      (uber_data[\"pickup_longitude\"] >= -74.242330) & \n",
    "                      (uber_data[\"pickup_latitude\"] <= 40.908524) & \n",
    "                      (uber_data[\"pickup_longitude\"] <= -73.717047) &\n",
    "                      (uber_data[\"dropoff_latitude\"] >= 40.560445) & \n",
    "                      (uber_data[\"dropoff_longitude\"] >= -74.242330) & \n",
    "                      (uber_data[\"dropoff_latitude\"] <= 40.908524) & \n",
    "                      (uber_data[\"dropoff_longitude\"] <= -73.717047)]\n",
    "    \n",
    "    # Checking if there are any null values for pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude\n",
    "    null_drop_lat = uber_data[uber_data['dropoff_latitude'].isnull()]\n",
    "    null_drop_long = uber_data[uber_data['dropoff_longitude'].isnull()]\n",
    "    null_pick_lat= uber_data[uber_data['pickup_latitude'].isnull()]\n",
    "    null_pick_long = uber_data[uber_data['pickup_longitude'].isnull()]\n",
    "\n",
    "    # Return True, if none of the colums have null values \n",
    "\n",
    "   # if null_drop_lat.empty & null_drop_long.empty & null_pick_lat.empty & null_pick_long.empty :\n",
    "        #print(True)\n",
    "    #else:\n",
    "       # print(False)\n",
    "\n",
    "    \n",
    "    # Removing rows where passamger count is 0 \n",
    "    uber_data = uber_data[uber_data['passenger_count']!=0]\n",
    "\n",
    "\n",
    "    # Removing rows with passanger data is abnormally large \n",
    "    uber_data = uber_data[uber_data['passenger_count']<=6]\n",
    "\n",
    "    # Checking datatypes for all columns \n",
    "    #print(uber_data.dtypes)\n",
    "\n",
    "    #Making sure pickup time is a datetime object and normalizing the name \n",
    "    uber_data ['pickup_time'] = pd.to_datetime(uber_data ['pickup_datetime'])\n",
    " \n",
    "\n",
    "\n",
    "    return uber_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_clean_uber_data(\"uber_rides_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We use the add distance column fcuntion we had defined before to add a new column with the distance \n",
    "of the ride to our uber data. We also drop columns where the distance of the ride is ==0\"\"\"\n",
    "\n",
    "def get_uber_data() -> pd.DataFrame:\n",
    "    uber_dataframe = load_and_clean_uber_data(\"uber_rides_sample.csv\")\n",
    "    add_distance_column(uber_dataframe)\n",
    "    uber_dataframe = uber_dataframe.drop(index=uber_dataframe[uber_dataframe['distance'] == 0].index)\n",
    "    return uber_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unnecessary columns \n",
    "final_uber_data = final_uber_data.drop('Unnamed: 0', axis=1)\n",
    "final_uber_data = final_uber_data.drop('key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_uber_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function takes all the weather files, iterates through them and merges them \n",
    "into one dataframe. The output is the combined dataframe\"\"\"\n",
    "\n",
    "def get_all_weather_csvs() -> pd.DataFrame:\n",
    "    years = list(range(2009, 2016))\n",
    "\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Iterate over the weather files\n",
    "    for year in years:\n",
    "        filepath = f\"{year}_weather.csv\"\n",
    "        df = pd.read_csv(filepath)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all the dataframes into a single dataframe\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function first loads the uber data from the csv file. \n",
    "We then filter based on coordinates to make sure the rides are within the coordinates we want.\n",
    "We also remove trips with 0 passangers and no fares. We further remove trips with passangers above 6 as that \n",
    "is uber policy. Lastly we remove trips with no distace between dropoff and pickup. The output is the\n",
    "cleaned dataframe\"\"\"\n",
    "\n",
    "def load_and_clean_weather_data() -> pd.DataFrame:\n",
    "\n",
    "    df = get_all_weather_csvs()\n",
    "\n",
    "    df1 = df[['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'NAME','HourlyPrecipitation','HourlyWindGustSpeed', 'HourlyWindSpeed', 'DailyAverageWindSpeed','DailyPrecipitation']]\n",
    "    df2 = df1.dropna(subset=['HourlyPrecipitation', 'HourlyWindGustSpeed'])\n",
    "\n",
    "    #column_types = df2.dtypes\n",
    "\n",
    "    #print(column_types)\n",
    "\n",
    "    # we see that the averages for wind speed and precipitation are null for all values so we can drop the columns \n",
    "\n",
    "    # We also doing need the hourly wind gust speed as we will be using the hourly wind speed, we can drop that column as well\n",
    "\n",
    "    df2 = df2.drop(columns=['DailyAverageWindSpeed','DailyPrecipitation', 'HourlyWindGustSpeed','LATITUDE', 'LONGITUDE'])\n",
    "    df2['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "    df2\n",
    "\n",
    "    # Removing all rows where Hourly preicipitation has the value \"T\" as we do not need to measure trace amounts \n",
    "\n",
    "    df3 = df2[df2['HourlyPrecipitation'] != \"T\"]\n",
    "\n",
    "    df4 = df3.drop(columns=[\"STATION\"])\n",
    "\n",
    "    df4 = df4.reset_index()\n",
    "\n",
    "    df4['DATE'] = df4['DATE'].apply(lambda x: x.to_pydatetime())\n",
    "\n",
    "    df4['DATE'] = pd.to_datetime(df4['DATE'])\n",
    "\n",
    "    df4['HourlyPrecipitation'] = df4['HourlyPrecipitation'].str.replace(r'(\\d+)\\s*[sS]$', r'\\1', regex=True)\n",
    "    \n",
    "    # convert column \"A\" from object to float\n",
    "    df4['HourlyPrecipitation'] = df4['HourlyPrecipitation'].astype(float)\n",
    "\n",
    "    Weather_Data = df4.drop('index', axis=1)\n",
    "\n",
    "    return  Weather_Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Roll up the data to daily\"\"\"\n",
    "def clean_month_weather_data_daily() -> pd.DataFrame:\n",
    "\n",
    "    daily_data = load_and_clean_weather_data()\n",
    "\n",
    "    daily_data_final = daily_data.groupby([daily_data['DATE'].dt.year, daily_data['DATE'].dt.month, daily_data['DATE'].dt.day]).sum()[['HourlyPrecipitation', \"HourlyWindSpeed\" ]]\n",
    "\n",
    "    daily_data_final = daily_data_final.rename_axis(index=['Year', 'Month', 'Day'])\n",
    "    \n",
    "    return daily_data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e6f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly() -> pd.DataFrame:\n",
    "\n",
    "    hourly_data = load_and_clean_weather_data()\n",
    "\n",
    "    hourly_data_final = hourly_data.groupby([hourly_data['DATE'].dt.year, hourly_data['DATE'].dt.month, hourly_data['DATE'].dt.day, hourly_data['DATE'].dt.hour]).sum()[['HourlyPrecipitation', \"HourlyWindSpeed\" ]]\n",
    "\n",
    "    hourly_data_final = hourly_data_final.rename_axis(index=['Year', 'Month', 'Day', 'Hour'])\n",
    "    \n",
    "    return hourly_data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data = clean_month_weather_data_hourly()\n",
    "hourly_weather_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data = clean_month_weather_data_daily()\n",
    "daily_weather_data = daily_weather_data.reset_index()\n",
    "\n",
    "daily_weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS HOURLY_WEATHER (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        year INTEGER,\n",
    "        month INTEGER,\n",
    "        day INTEGER,\n",
    "        hour INTEGER,\n",
    "        precipitation REAL,\n",
    "        wind REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS DAILY_WEATHER (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        year INTEGER,\n",
    "        month INTEGER,\n",
    "        day INTEGER,\n",
    "        precipitation REAL,\n",
    "        wind REAL\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS TAXI_TRIPS (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        pickup_datetime TEXT,\n",
    "        dropoff_datetime TEXT,\n",
    "        Passenger_Count REAL\n",
    "        Trip_Distance REAL,\n",
    "        Start_Lon REAL,\n",
    "        Start_Lat REAL,\n",
    "        End_Lon REAL, \n",
    "        End_Lat REAL,\n",
    "        Fare_Amt REAL, \n",
    "        Tip_Amt REAL, \n",
    "        Total_Amt REAL,\n",
    "        distance REAL,\n",
    "\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS UBER_TRIPS (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        pickup_datetime TEXT,\n",
    "        pickup_longitude REAL,\n",
    "        pickup_latitude REAL,\n",
    "        dropoff_longitude REAL,\n",
    "        dropoff_latitude REAL,\n",
    "        fare_amount REAL,\n",
    "        distance REAL,\n",
    "        passenger_count INTEGER,\n",
    "    );\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table():\n",
    "\n",
    "    hourly_weather_data.to_sql(name='HOURLY_WEATHER', con=engine, if_exists='replace', index=False)\n",
    "    daily_weather_data.to_sql(name='DAILY_WEATHER', con=engine, if_exists='replace', index=False)\n",
    "    final_uber_data.to_sql(name='UBER_TRIPS', con=engine, if_exists='replace', index=False)\n",
    "    Taxi_Data.to_sql(name='TAXI_TRIPS', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "    \n",
    "write_dataframes_to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e495251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# establish a connection to the SQL database\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# execute a SELECT query on the HOURLY_WEATHER table\n",
    "query = \"SELECT * FROM TAXI_TRIPS LIMIT 10;\"\n",
    "result = engine.execute(query)\n",
    "\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# establish a connection to the SQL database\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# execute a SELECT query on the HOURLY_WEATHER table\n",
    "\n",
    "with open('1_hour_day.sql', 'r') as file:\n",
    "    query = file.read()\n",
    "\n",
    "result = engine.execute(query)\n",
    "\n",
    "\n",
    "for row in result:\n",
    "    print(row)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3725eb44",
   "metadata": {},
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fbf5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# establish a connection to the SQL database\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# execute a SELECT query on the HOURLY_WEATHER table\n",
    "\n",
    "with open('2_day_week.sql', 'r') as file:\n",
    "    query = file.read()\n",
    "\n",
    "result = engine.execute(query)\n",
    "\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d53f383c",
   "metadata": {},
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# establish a connection to the SQL database\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# execute a SELECT query on the HOURLY_WEATHER table\n",
    "\n",
    "with open('3_95_percentile.sql', 'r') as file:\n",
    "    query = file.read()\n",
    "\n",
    "result = engine.execute(query)\n",
    "\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59638822",
   "metadata": {},
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# establish a connection to the SQL database\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# execute a SELECT query on the HOURLY_WEATHER table\n",
    "\n",
    "with open('4_top_10_days.sql', 'r') as file:\n",
    "    query = file.read()\n",
    "\n",
    "result = engine.execute(query)\n",
    "\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12a1fc3e",
   "metadata": {},
   "source": [
    "### Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c70c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# establish a connection to the SQL database\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# execute a SELECT query on the HOURLY_WEATHER table\n",
    "\n",
    "with open('5_10_windiest_days.sql', 'r') as file:\n",
    "    query = file.read()\n",
    "\n",
    "result = engine.execute(query)\n",
    "\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5b5a047",
   "metadata": {},
   "source": [
    "### Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"\n",
    "    SELECT strftime('%w', pickup_datetime) AS day_of_week, COUNT(*) AS frequency\n",
    "    FROM UBER_TRIPS\n",
    "    WHERE pickup_datetime BETWEEN '2009-01-01 00:00:00 UTC' AND '2015-06-30 23:59:59 UTC'\n",
    "    GROUP BY day_of_week\n",
    "    ORDER BY day_of_week;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frequency_hour(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    hour = [1, 2, 3, 4, 5]\n",
    "    values = [1, 5, 3, 2, 5]\n",
    "\n",
    "    axes.bar(hour, values)\n",
    "\n",
    "    axes.set_ylabel('Popularity')\n",
    "    axes.set_xlabel('Hour')\n",
    "    axes.set_title(\"Frequency per Hour\")\n",
    "    axes.set_xlim(-1, 11)\n",
    "    axes.set_ylim(-1.5, 1.5)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_frequency_hour():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_frequency_hour()\n",
    "plot_frequency_hour(some_dataframe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c350a277",
   "metadata": {},
   "source": [
    "### Visualization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02eeb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_distance_month(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(30, 20))\n",
    "    \n",
    "    month = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "    distance = [1, 5, 3, 2, 5, 7, 8, 1, 9, 23, 6, 7]\n",
    "\n",
    "    ci = 1.90 * np.std(distance)/np.sqrt(len(month))\n",
    "\n",
    "    axes.plot(month,distance)\n",
    "    axes.plot(month, distance, 'o', color='tab:brown')\n",
    "    \n",
    "    axes.fill_between(month, (distance - ci), (distance + ci), color= 'b', alpha = 0.1)\n",
    "\n",
    "    axes.set_ylabel('Average Distance')\n",
    "    axes.set_xlabel('Month')\n",
    "    axes.set_title(\"Average Distance per Month\")\n",
    "\n",
    "\n",
    "    # a, b = np.polyfit(x, y, deg=1)\n",
    "    # y_est = a * x + b\n",
    "    # y_err = x.std() * np.sqrt(1/len(x) +\n",
    "    #                         (x - x.mean())**2 / np.sum((x - x.mean())**2))\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    # ax.plot(x, y_est, '-')\n",
    "    # ax.fill_between(x, y_est - y_err, y_est + y_err, alpha=0.2)\n",
    "    # ax.plot(x, y, 'o', color='tab:brown');\n",
    "\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_avg_distance_month():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_avg_distance_month()\n",
    "plot_avg_distance_month(some_dataframe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "275a1af2",
   "metadata": {},
   "source": [
    "### Visualization 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e60fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dropoffs_ny_area(dataframe):\n",
    "\n",
    "    # Click boxes on plot to see animation + which day of the week was most popular for drop offs for each airport\n",
    "\n",
    "    map_obj = folium.Map(location = [40.730610, -73.935242], zoom_start = 11, min_zoom = 11, tiles='CartoDB positron')\n",
    "\n",
    "    folium.Rectangle([(40.778865,-73.854838), (40.763589,-73.891745)], fill=True, fill_color='#ff7800', fill_opacity=0.2).add_child(folium.Popup('Tuesday')).add_to(map_obj) # LGA\n",
    "\n",
    "    folium.Rectangle([(40.651376, -73.766264), (40.639263, -73.795642)], fill=True, fill_color='#ff7800', fill_opacity=0.2).add_child(folium.Popup('Tuesday')).add_to(map_obj) # JFK\n",
    "\n",
    "    folium.Rectangle([(40.699680, -74.165205), (40.686794, -74.194028)], fill=True, fill_color='#ff7800', fill_opacity=0.2).add_child(folium.Popup('Tuesday')).add_to(map_obj) # EWR\n",
    "    \n",
    "    return map_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dropoffs_ny_area():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_dropoffs_ny_area()\n",
    "plot_dropoffs_ny_area(some_dataframe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e085f33",
   "metadata": {},
   "source": [
    "### Visualization 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trips_area(dataframe):    \n",
    "    map_obj = folium.Map(location = [40.730610, -73.935242], zoom_start = 10, min_zoom = 10, tiles='CartoDB positron')\n",
    "\n",
    "    lats_longs = [\n",
    "                    [40.7554, -73.9862],\n",
    "                    [40.7794, -73.9654],\n",
    "                    [40.7223, -73.9982],\n",
    "                    [40.7455, -74.0071],\n",
    "                ]\n",
    "\n",
    "    HeatMap(lats_longs).add_to(map_obj)\n",
    "    return map_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_trips_area():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_trips_area()\n",
    "plot_trips_area(some_dataframe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9029b60",
   "metadata": {},
   "source": [
    "### Visualization 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdcd1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tips_distance(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    tips = [2, 6, 3, 7, 8, 1, 9, 22, 9, 6, 22, 1]\n",
    "    distance = [1, 5, 3, 2, 5, 7, 8, 1, 9, 23, 6, 7]\n",
    "\n",
    "    axes.scatter(distance, tips, marker='o', alpha=0.5)\n",
    "    axes.set_title(\"Yellow Tips - Tips vs. Distance\")\n",
    "    axes.set_ylabel('Popularity')\n",
    "    axes.set_xlabel('Distance')\n",
    "    axes.set_xlim(-1, 11)\n",
    "    axes.set_ylim(-1, 10)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tips_distance():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_tips_distance()\n",
    "plot_tips_distance(some_dataframe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "410cfcdd",
   "metadata": {},
   "source": [
    "### Visualization 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tips_precipitation(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    tips = [2, 6, 3, 7, 8, 1, 9, 22, 9, 6, 22, 1]\n",
    "    precipication = [1, 5, 3, 2, 5, 7, 8, 1, 9, 23, 6, 7]\n",
    "\n",
    "    axes.scatter(precipication, tips, marker='o', alpha=0.5)\n",
    "    axes.set_title(\"Yellow Taxi - Tips vs. Precipitation\")\n",
    "    axes.set_ylabel('Precipitation')\n",
    "    axes.set_xlabel('Tips')\n",
    "    axes.set_xlim(-1, 11)\n",
    "    axes.set_ylim(-1, 10)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tips_precipitation():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_tips_precipitation()\n",
    "plot_tips_precipitation(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d659ea492a6423f437f8c825d2ef59ba06a7f23250fb4bfa35b0a006bd446c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
